{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Dw0HhcrKLwrG"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Simplified Notebook for SentimentArcs\n",
        "\n",
        "Created:\n",
        "\n",
        "* 28 Oct 2022\n",
        "* Jon Chun\n",
        "\n",
        "Simplified version of SentimentArcs Notebooks:\n",
        "\n",
        "* https://github.com/jon-chun/sentimentarcs_notebooks\n",
        "\n",
        "* https://arxiv.org/pdf/2110.09454.pdf"
      ],
      "metadata": {
        "id": "YwxnxnOXF4TB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Review VM Specs"
      ],
      "metadata": {
        "id": "9O8gnSf4iIw9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure your Linux VM is connected to a GPU\n",
        "\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "NSZ4PRXtiIw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Node GPU Count/Type\n",
        "\n",
        "!nvidia-smi -L"
      ],
      "metadata": {
        "id": "FsMiuonMiIw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Memory\n",
        "\n",
        "!free -h --si | awk  '/Mem:/{print $2}'"
      ],
      "metadata": {
        "id": "W6cw1NyMiIw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU log information\n",
        "\n",
        "# !nvidia-smi -q"
      ],
      "metadata": {
        "id": "Hir6o2DAiIw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check how many CPU cores available for parallization\n",
        "\n",
        "!cat /proc/cpuinfo"
      ],
      "metadata": {
        "id": "s2waXTRWiIw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!lscpu"
      ],
      "metadata": {
        "id": "WW-K8tVviIw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "84IZ26RpiS7f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Libraries"
      ],
      "metadata": {
        "id": "lVKek_uU7NG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[sentencepiece]\n",
        "\n",
        "# !pip install transformers"
      ],
      "metadata": {
        "id": "Dd4CiRWNof2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# May require [RESET RUNTIME]\n",
        "\n",
        "# !pip install modin[all]"
      ],
      "metadata": {
        "id": "GEHDAX84QeDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries"
      ],
      "metadata": {
        "id": "w6AarbSqQweP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files"
      ],
      "metadata": {
        "id": "7asUI7bIT0Zr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "# import modin.pandas as pd_modin\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import datetime\n",
        "import re\n",
        "import os\n",
        "\n",
        "from tqdm import tqdm\n",
        "import tqdm.notebook as tq\n",
        "# for i in tq.tqdm(...):"
      ],
      "metadata": {
        "id": "2vBv9P-fZaxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure Settings"
      ],
      "metadata": {
        "id": "9YGBQuOQ8262"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\""
      ],
      "metadata": {
        "id": "aex0daSTdhQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Global Variables & Functions"
      ],
      "metadata": {
        "id": "NdyvMBNkiYtk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Global Variables"
      ],
      "metadata": {
        "id": "nDRQt-CWditY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT_ENCODING = 'utf-8'"
      ],
      "metadata": {
        "id": "MMyh_0OMIJcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main (Modin) DataFrame for Novel Sentiments\n",
        "\n",
        "sentiment_df = pd.DataFrame\n"
      ],
      "metadata": {
        "id": "Qb_U-MrR-2x0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Common Functions"
      ],
      "metadata": {
        "id": "EMhPyO0F85GJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def verify_novel(novel_str, index_ends=500):\n",
        "  '''\n",
        "  INPUT: string in some stage of processing\n",
        "  OUTPUT: display summary index_ends chars of header/footer for verification\n",
        "  '''\n",
        "\n",
        "  print(f'Novel Name: {novel_name_str}')\n",
        "  print(f'  Char Len: {len(novel_str)}')\n",
        "  print('====================================\\n')\n",
        "  print(f'Beginning:\\n\\n {novel_str[:index_ends]}\\n\\n')\n",
        "  print('\\n------------------------------------')\n",
        "  print(f'Ending:\\n\\n {novel_str[-index_ends:]}\\n\\n')"
      ],
      "metadata": {
        "id": "DmBgCGUCDYGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_text2txt_and_download(text_obj, file_suffix='_save.txt'):\n",
        "  '''\n",
        "  INPUT: text object and suffix to add to output text filename\n",
        "  OUTPUT: Write text object to text file (both temp VM and download)\n",
        "  '''\n",
        "\n",
        "  if type(text_obj) == str:\n",
        "    print('STEP 1. Processing String Object\\n')\n",
        "    str_obj = text_obj\n",
        "  elif type(text_obj) == list:\n",
        "    if (len(text_obj) > 0):\n",
        "      if type(text_obj[0]) == str:\n",
        "        print('STEP 1. Processing List of Strings Object\\n')\n",
        "        str_obj = \"\\n\".join(text_obj)\n",
        "      else:\n",
        "        print('ERROR: Object is not an List of Strings [save_text2txt_and_download()]')\n",
        "        return -1\n",
        "    else:\n",
        "      print('ERROR: Object is an empty List [save_text2txt_and_download()]')\n",
        "      return -1\n",
        "  else:\n",
        "    print('ERROR: Object Type is neither String nor List [save_text2txt_and_download()]')\n",
        "    return -1\n",
        "\n",
        "  datetime_str = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
        "  out_filename = novel_name_str.split('.')[0] + '_' + datetime_str + file_suffix\n",
        "\n",
        "  # Write file to temporary VM filesystem\n",
        "  print(f'STEP 2. Saving textfile to temporary VM file: {out_filename}\\n')\n",
        "  with open(out_filename, \"w\") as fp:\n",
        "    fp.write(str_obj)\n",
        "\n",
        "  # Download permanent copy of file\n",
        "  print(f'STEP 3. Downloading permanent copy of textfile: {out_filename}\\n')\n",
        "  files.download(out_filename)"
      ],
      "metadata": {
        "id": "4284uzStHpyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_df2csv_and_download(df_obj, file_suffix='_save.csv', nodate=True):\n",
        "  '''\n",
        "  INPUT: DataFrame object and suffix to add to output csv filename\n",
        "  OUTPUT: Write DataFrame object to csv file (both temp VM and download)\n",
        "  '''\n",
        "\n",
        "  if isinstance(df_obj, pd.DataFrame):\n",
        "    datetime_str = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
        "    if nodate:\n",
        "      out_filename = novel_name_str.split('.')[0] + file_suffix\n",
        "    else:\n",
        "      out_filename = novel_name_str.split('.')[0] + '_' + datetime_str + file_suffix\n",
        "    # print(f'STEP 1. Saving DataFrame: {df_obj.__name__} to temporary VM file: {out_filename}\\n') # Also, isinstance(obj, pd.DataFrame)\n",
        "    print(f'STEP 1. Saving DataFrame to temporary VM file: {out_filename}\\n')\n",
        "    df_obj.to_csv(out_filename, index=False) \n",
        "  else:\n",
        "    print(f'ERROR: Object is not a DataFrame [save_df2csv_and_download()]')\n",
        "    return -1\n",
        "\n",
        "  # Download permanent copy of file\n",
        "  print(f'STEP 2. Downloading permanent copy of csvfile: {out_filename}\\n')\n",
        "  files.download(out_filename)\n",
        "\n",
        "\n",
        "# Test\n",
        "\n",
        "# save_df2csv_and_download(temp_df, '_bert-nlptown.txt')"
      ],
      "metadata": {
        "id": "CeA0VLNT56TZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Clean Text"
      ],
      "metadata": {
        "id": "kqLNZq62HImu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Option (a): Clean Text"
      ],
      "metadata": {
        "id": "vkYsds6MCIJK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Upload Raw Text File\n",
        "\n",
        "Get plain text of familiar novel at:\n",
        "* https://gutenberg.net.au/ (AUS)\n",
        "* https://gutenberg.org/ (US)"
      ],
      "metadata": {
        "id": "2YVNpxpABMoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# NOTE: 1m07s\n",
        "\n",
        "# Upload Plain Text File\n",
        "novel_name_str = ''\n",
        "uploaded = files.upload()\n",
        "\n",
        "# NOTE: Allows for multiple file uploads, will only process the last\n",
        "#       Left in for future feature addition (processing multiple files at once)\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "  novel_name_str = fn\n",
        "\n",
        "# Extract from Dict and decode binary into char string\n",
        "novel_raw_str = uploaded[novel_name_str].decode(TEXT_ENCODING)"
      ],
      "metadata": {
        "id": "R3WqS11FA3GY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify\n",
        "\n",
        "verify_novel(novel_raw_str)"
      ],
      "metadata": {
        "id": "KRiLBvvACAJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clean Text"
      ],
      "metadata": {
        "id": "Sw5cff-kNOBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install clean-text"
      ],
      "metadata": {
        "id": "-mWoG6U07jU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unidecode  # clean-text dependency"
      ],
      "metadata": {
        "id": "w8TQCzw6ZjTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from cleantext import clean"
      ],
      "metadata": {
        "id": "Tnbwxio8NFs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "novel_clean_str = clean(novel_raw_str,\n",
        "    fix_unicode=True,               # fix various unicode errors\n",
        "    to_ascii=True,                  # transliterate to closest ASCII representation\n",
        "    lower=True,                     # lowercase text\n",
        "    no_line_breaks=False,           # fully strip line breaks as opposed to only normalizing them\n",
        "    no_urls=False,                  # replace all URLs with a special token\n",
        "    no_emails=False,                # replace all email addresses with a special token\n",
        "    no_phone_numbers=False,         # replace all phone numbers with a special token\n",
        "    no_numbers=False,               # replace all numbers with a special token\n",
        "    no_digits=False,                # replace all digits with a special token\n",
        "    no_currency_symbols=False,      # replace all currency symbols with a special token\n",
        "    no_punct=False,                 # remove punctuations\n",
        "    # replace_with_punct=\"\",          # instead of removing punctuations you may replace them\n",
        "    # replace_with_url=\"<URL>\",\n",
        "    # replace_with_email=\"<EMAIL>\",\n",
        "    # replace_with_phone_number=\"<PHONE>\",\n",
        "    # replace_with_number=\"<NUMBER>\",\n",
        "    # replace_with_digit=\"0\",\n",
        "    # replace_with_currency_symbol=\"<CUR>\",\n",
        "    lang=\"en\"                       # set to 'de' for German special handling\n",
        ")\n",
        "\n",
        "# Replace all new lines/returns with single whitespace\n",
        "novel_clean_str = novel_clean_str.replace('\\n\\r', ' ')\n",
        "novel_clean_str = novel_clean_str.replace('\\n', ' ')\n",
        "novel_clean_str = novel_clean_str.replace('\\r', ' ')\n",
        "novel_clean_str = ' '.join(novel_clean_str.split())\n",
        "novel_clean_str "
      ],
      "metadata": {
        "id": "TRTdmakW8tO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify\n",
        "\n",
        "verify_novel(novel_clean_str, index_ends=500)"
      ],
      "metadata": {
        "id": "GIsI8CgED8N7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [CAUTION] Trim Header & Footer\n",
        "\n",
        "**CAUTION:** This requires manually adjusting the RegEx expressions to identify boundries between the header-novel (header_end_re) and the novel-footer (footer_start_re).\n",
        "\n",
        "It is usually faster and more efficient to manually download, trim header/footer and upload a clean plain text file than use this procedure."
      ],
      "metadata": {
        "id": "L3nDeWpUNLoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RegEx to trip header and footer\n",
        "\n",
        "# RegEx for End of Header\n",
        "header_end_re = r'*** START OF THE PROJECT GUTENBERG EBOOK THE IDIOT ***'\n",
        "# header_end_re = r'Towards the end of November, during a thaw'\n",
        "\n",
        "# RegEx for Start of Footer\n",
        "footer_start_re = r'*** END OF THE PROJECT GUTENBERG EBOOK THE IDIOT ***'\n",
        "# footer_start_re = r'as she took leave of Evgenie Pavlovitch'"
      ],
      "metadata": {
        "id": "BabYrXvN6KaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to extract\n",
        "def trim_header_footer(text_str, aheader_end_re, afooter_start_re):\n",
        "  '''\n",
        "  INPUT: Given a long text string consisting of: [Header] + [Novel] + [Footer]\n",
        "  OUTPUT: Return just the [Novel] text string\n",
        "  '''\n",
        "\n",
        "  # Discards the metadata from the beginning of the book\n",
        "  # index_start = re.search(r\"\\*\\*\\* START OF THIS PROJECT GUTENBERG EBOOK .* \\*\\*\\*\",raw ).end()\n",
        "  header_end_index = re.search(re.escape(aheader_end_re), text_str, re.IGNORECASE).end()\n",
        "  # header_end_index = re.search(r'*** START OF THE PROJECT GUTENBERG EBOOK THE IDIOT ***', novel_raw_str).end()\n",
        "\n",
        "  # Discards the metadata from the end of the book\n",
        "  footer_start_index = re.search(re.escape(afooter_start_re), text_str, re.IGNORECASE).start()\n",
        "  # footer_start_index = re.search(r'*** END OF THE PROJECT GUTENBERG EBOOK THE IDIOT ***', novel_raw_str).start()\n",
        "\n",
        "  # Keeps the relevant text\n",
        "  novel_trim_str = text_str[header_end_index:footer_start_index]\n",
        "\n",
        "  return novel_trim_str"
      ],
      "metadata": {
        "id": "dhJfbLChBoBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trim Header and Footer\n",
        "\n",
        "novel_trim_str = trim_header_footer(novel_raw_str, header_end_re, footer_start_re)\n",
        "print(f'    Length (Raw): {len(novel_raw_str)}')\n",
        "print(f'Length (Trimmed): {len(novel_trim_str)}')"
      ],
      "metadata": {
        "id": "3zJ9JeI0BoEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify\n",
        "\n",
        "verify_novel(novel_trim_str, 500)"
      ],
      "metadata": {
        "id": "e31VZIKFEIMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Segment Text"
      ],
      "metadata": {
        "id": "qDa43-cGOIMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pysbd  # Python Sentence Boundry Detection"
      ],
      "metadata": {
        "id": "BJqpPixhOtJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pysbd"
      ],
      "metadata": {
        "id": "OB27gvYJOxnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FIX: Normally assigned within the 'Trim Header & Footer' Section\n",
        "\n",
        "# Ensure we have trimmed version of novel in novel_trim_str\n",
        "\n",
        "if len(novel_trim_str) > 0:\n",
        "  # Header/Footer already trimmed from body of Novel\n",
        "  pass\n",
        "else:\n",
        "  novel_trim_str = novel_raw_str"
      ],
      "metadata": {
        "id": "wz6aVlxzbtEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# NOTE: 1m05s\n",
        "\n",
        "# Split Novel into Segments (~Sentences)\n",
        "seg = pysbd.Segmenter(language=\"en\", clean=False)\n",
        "novel_segments_ls = seg.segment(novel_trim_str)"
      ],
      "metadata": {
        "id": "JhSURloROtF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trim any leading/trailing whitespace on all Sentences\n",
        "\n",
        "novel_clean_ls = [x.strip() for x in novel_segments_ls]"
      ],
      "metadata": {
        "id": "19xTLVlAPjlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify\n",
        "\n",
        "verify_novel(novel_clean_ls, 10)"
      ],
      "metadata": {
        "id": "62DhkoOkEauJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save to file and download copy\n",
        "\n",
        "save_text2txt_and_download(novel_clean_ls, '_segments.txt')"
      ],
      "metadata": {
        "id": "EWECS98kFM1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_df"
      ],
      "metadata": {
        "id": "gl_jxnxWK9ED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Populate novel sentiment_df with sentence number and clean segmented strings\n",
        "\n",
        "sentence_no_ls = list(range(len(novel_clean_ls)))\n",
        "sentence_no_ls[-1]\n",
        "\n",
        "sentiment_df = pd.DataFrame({'line_no':sentence_no_ls, 'line':novel_clean_ls})\n",
        "sentiment_df.head()"
      ],
      "metadata": {
        "id": "m5Qbh6uHJ_fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Option (b): Read Clean Text from File"
      ],
      "metadata": {
        "id": "HG-GtwKxFZUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "0X44VMvgFfyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_df.head()"
      ],
      "metadata": {
        "id": "N5MAoIVDFubY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 10 TheIdiot_FyodorDostoyevsky_GutenbergOrg_20221028-031127__vader.csv"
      ],
      "metadata": {
        "id": "lKlzv7z3GkQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "saved_vader_csv = 'TheIdiot_FyodorDostoyevsky_GutenbergOrg_vader.csv'\n",
        "\n",
        "sentiment_df = pd.read_csv(saved_vader_csv, index_col=[0])\n",
        "sentiment_df.drop(columns=['vader'], inplace=True)\n",
        "\n",
        "novel_clean_ls = sentiment_df.line.to_list()\n",
        "\n",
        "sentiment_df.head()"
      ],
      "metadata": {
        "id": "RylUx5mYFkD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compute Sentiment"
      ],
      "metadata": {
        "id": "2thPN2r_QJSz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Option (1): Symbolic: Lexicons"
      ],
      "metadata": {
        "id": "rJUoLqF_QWbw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VADER"
      ],
      "metadata": {
        "id": "-9C-x51-LaES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vaderSentiment"
      ],
      "metadata": {
        "id": "zJ2XMJDSF3jU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "sid_obj = SentimentIntensityAnalyzer()"
      ],
      "metadata": {
        "id": "Qma7eu0gQnKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_vader_ls = [sid_obj.polarity_scores(asentence)['compound'] for asentence in novel_clean_ls]"
      ],
      "metadata": {
        "id": "Lng_pNpVQnHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new SyuzhetR DataFrame to save results\n",
        "\n",
        "vader_df = sentiment_df[['line_no', 'line']].copy(deep=True)\n",
        "vader_df['vader'] = pd.Series(sentiment_vader_ls) \n",
        "vader_df.head()"
      ],
      "metadata": {
        "id": "ckLEiP79jwRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "win_per = 0.1\n",
        "win_size = int(win_per * sentiment_df.shape[0])\n",
        "\n",
        "_ = vader_df['vader'].rolling(win_size, center=True).mean().plot(grid=True)"
      ],
      "metadata": {
        "id": "iQXLeEFP_pt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save VADER Model Sentiment Time Series\n",
        "\n",
        "save_df2csv_and_download(vader_df, '_vader.csv', nodate=True)"
      ],
      "metadata": {
        "id": "_hf9ZTDTUxU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SyuzhetR (4)\n",
        "\n",
        "* SyzuhetR: https://cran.r-project.org/web/packages/readtext/vignettes/readtext_vignette.html\n",
        "* http://rstudio-pubs-static.s3.amazonaws.com/283881_efbb666d653a4eb3b0c5e5672e3446c6.html\n",
        "\n",
        "* SentimentR: https://github.com/trinker/sentimentr\n",
        "\n",
        "* JupyterLab w/Py OR R: https://www.youtube.com/watch?v=Q35WIqZoUF4"
      ],
      "metadata": {
        "id": "LwstY7VcCixU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext rpy2.ipython"
      ],
      "metadata": {
        "id": "yCEwjcx3wNkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Python libraries to exchange data with R Program Space and read R Datafiles\n",
        "\n",
        "import rpy2.robjects as robjects\n",
        "from rpy2.robjects.packages import importr"
      ],
      "metadata": {
        "id": "5nny0fWmWhBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%R getwd()"
      ],
      "metadata": {
        "id": "UTVTOkZmwNdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%R list.files()"
      ],
      "metadata": {
        "id": "IaTCr6yJVhRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time \n",
        "%%capture \n",
        "%%R\n",
        "\n",
        "# Install Syuzhet.R, Sentiment.R and Utility Libraries\n",
        "\n",
        "# NOTE: 56s 17:30EST on 27Oct2022 - Colab Pro\n",
        "\n",
        "install.packages(c('syuzhet', 'sentimentr', 'tidyverse', 'lexicon'))\n",
        "\n",
        "library(syuzhet)\n",
        "library(sentimentr)\n",
        "library(tidyverse)\n",
        "library(lexicon)"
      ],
      "metadata": {
        "id": "azOwcqnQV4ee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%R sessionInfo()"
      ],
      "metadata": {
        "id": "3tp5BKpeWM6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_df.head()"
      ],
      "metadata": {
        "id": "2fSFFsNcWu6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Compute Sentiments from all 4 Syuzhet Models\n",
        "\n",
        "# NOTE:  3m57s 17:40EST on 27Oct2022 Colab Pro (The Idiot)\n",
        "#        3m55s 18:02EST on 27Oct2022 Colab Pro (The Idiot)\n",
        "#        4m10s 23:14EST on 27Oct2022 Colab Pro (The Idiot)\n",
        "\n",
        "syuzhet = importr('syuzhet')\n",
        "\n",
        "# Create new SyuzhetR DataFrame to save results\n",
        "syuzhet_df = sentiment_df[['line_no', 'line']].copy(deep=True)\n",
        "\n",
        "print('[1/4] Processing syuzhetr_syuzhet')\n",
        "syuzhet_df['syuzhetr_syuzhet'] = syuzhet.get_sentiment(syuzhet_df['line'].to_list(), method='syuzhet')\n",
        "print('[2/4] Processing syuzhetr_bing')\n",
        "syuzhet_df['syuzhetr_bing'] = syuzhet.get_sentiment(syuzhet_df['line'].to_list(), method='bing')\n",
        "print('[3/4] Processing syuzhetr_afinn')\n",
        "syuzhet_df['syuzhetr_afinn'] = syuzhet.get_sentiment(syuzhet_df['line'].to_list(), method='afinn')\n",
        "print('[4/4] Processing syuzhetr_nrc')\n",
        "syuzhet_df['syuzhetr_nrc'] = syuzhet.get_sentiment(syuzhet_df['line'].to_list(), method='nrc')\n",
        "\n",
        "syuzhet_df.head()"
      ],
      "metadata": {
        "id": "K8c99c-RWaDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "win_per = 0.1\n",
        "win_size = int(win_per * syuzhet_df.shape[0])\n",
        "\n",
        "syuzhet_model_ls = ['syuzhetr_syuzhet', 'syuzhetr_bing', 'syuzhetr_afinn', 'syuzhetr_nrc']\n",
        "_ = syuzhet_df[syuzhet_model_ls].rolling(win_size, center=True).mean().plot(figsize=(12,6), grid=True)"
      ],
      "metadata": {
        "id": "eOio1DlHZQcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save SyuzhetR Models' Sentiment Time Series\n",
        "\n",
        "save_df2csv_and_download(syuzhet_df, '_syuzhetr.csv', nodate=True)"
      ],
      "metadata": {
        "id": "XE_CyXh8WZ_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SentimentR (8)\n",
        "\n",
        "Call function in external get_sentimentr.R from within Python Loop\n",
        "\n",
        "* https://medium.com/analytics-vidhya/calling-r-from-python-magic-of-rpy2-d8cbbf991571\n",
        "\n",
        "* https://rpy2.github.io/doc/v3.0.x/html/generated_rst/pandas.html"
      ],
      "metadata": {
        "id": "tURoX7JiYSMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file get_sentimentr.R\n",
        "\n",
        "library(sentimentr)\n",
        "library(lexicon)\n",
        "\n",
        "get_sentimentr_values <- function(s_v) {\n",
        "  \n",
        "  print('[1/8] Processing sentimentr_jockersrinker')\n",
        "  sentimentr_jockersrinker <- sentiment(s_v, polarity_dt=lexicon::hash_sentiment_jockers_rinker, \n",
        "                                        hypen=\"\", amplifier.weight=0.8, n.before=5, n.after=2,\n",
        "                                        adversative.weight=0.25, neutral.nonverb.like = FALSE, missing_value = 0)\n",
        "\n",
        "  print('[2/8] Processing sentimentr_jockers')\n",
        "  sentimentr_jockers <- sentiment(s_v, polarity_dt=lexicon::hash_sentiment_jockers, \n",
        "                                        hypen=\"\", amplifier.weight=0.8, n.before=5, n.after=2,\n",
        "                                        adversative.weight=0.25, neutral.nonverb.like = FALSE, missing_value = 0)\n",
        "\n",
        "  print('[3/8] Processing sentimentr_huliu')\n",
        "  sentimentr_huliu <- sentiment(s_v, polarity_dt=lexicon::hash_sentiment_huliu, \n",
        "                                        hypen=\"\", amplifier.weight=0.8, n.before=5, n.after=2,\n",
        "                                        adversative.weight=0.25, neutral.nonverb.like = FALSE, missing_value = 0)\n",
        "\n",
        "  print('[4/8] Processing sentimentr_nrc')\n",
        "  sentimentr_nrc <- sentiment(s_v, polarity_dt=lexicon::hash_sentiment_nrc, \n",
        "                                        hypen=\"\", amplifier.weight=0.8, n.before=5, n.after=2,\n",
        "                                        adversative.weight=0.25, neutral.nonverb.like = FALSE, missing_value = 0)\n",
        "\n",
        "  print('[5/8] Processing sentimentr_senticnet')\n",
        "  sentimentr_senticnet <- sentiment(s_v, polarity_dt=lexicon::hash_sentiment_senticnet, \n",
        "                                        hypen=\"\", amplifier.weight=0.8, n.before=5, n.after=2,\n",
        "                                        adversative.weight=0.25, neutral.nonverb.like = FALSE, missing_value = 0)\n",
        "\n",
        "  print('[6/8] Processing sentimentr_sentiword')\n",
        "  sentimentr_sentiword <- sentiment(s_v, polarity_dt=lexicon::hash_sentiment_sentiword, \n",
        "                                        hypen=\"\", amplifier.weight=0.8, n.before=5, n.after=2,\n",
        "                                        adversative.weight=0.25, neutral.nonverb.like = FALSE, missing_value = 0)\n",
        "\n",
        "  print('[7/8] Processing sentimentr_loughran_mcdonald')\n",
        "  sentimentr_loughran_mcdonald <- sentiment(s_v, polarity_dt=lexicon::hash_sentiment_loughran_mcdonald, \n",
        "                                        hypen=\"\", amplifier.weight=0.8, n.before=5, n.after=2,\n",
        "                                        adversative.weight=0.25, neutral.nonverb.like = FALSE, missing_value = 0)\n",
        "\n",
        "  print('[8/8] Processing sentimentr_socal_google')\n",
        "  sentimentr_socal_google <- sentiment(s_v, polarity_dt=lexicon::hash_sentiment_socal_google, \n",
        "                                        hypen=\"\", amplifier.weight=0.8, n.before=5, n.after=2,\n",
        "                                        adversative.weight=0.25, neutral.nonverb.like = FALSE, missing_value = 0)\n",
        "\n",
        "  anovel_sentimentr_df <- data.frame(# 'text_clean' = s_v,\n",
        "                                'sentimentr_jockersrinker' = sentimentr_jockersrinker$sentiment,\n",
        "                                'sentimentr_jockers' = sentimentr_jockers$sentiment,\n",
        "                                'sentimentr_huliu' = sentimentr_huliu$sentiment,\n",
        "                                'sentimentr_nrc' = sentimentr_nrc$sentiment,\n",
        "                                'sentimentr_senticnet' = sentimentr_senticnet$sentiment,\n",
        "                                'sentimentr_sentiword' = sentimentr_sentiword$sentiment,\n",
        "                                'sentimentr_loughran_mcdonald' = sentimentr_loughran_mcdonald$sentiment,\n",
        "                                'sentimentr_socal_google' = sentimentr_socal_google$sentiment\n",
        "                                )\n",
        "  return(anovel_sentimentr_df)\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "EB8hSEe2YR_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify the *.R file above was written correctly\n",
        "\n",
        "# !cat get_sentimentr.R"
      ],
      "metadata": {
        "id": "2Hp4J1HVYgv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup python robject with external library::function()\n",
        "# https://rpy2.github.io/doc/v3.0.x/html/generated_rst/pandas.html\n",
        "\n",
        "# import rpy2.robjects as robjects\n",
        "\n",
        "# Defining the R script and loading the instance in Python\n",
        "# from rpy2.robjects import pandas2ri \n",
        "r = robjects.r\n",
        "\n",
        "# Loading the function we have defined in R.\n",
        "r['source']('get_sentimentr.R')\n",
        "\n",
        "# Reading and processing data\n",
        "get_sentimentr_function_r = robjects.globalenv['get_sentimentr_values']"
      ],
      "metadata": {
        "id": "9YXY3mZHYgsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# NOTE:   2m40s  @17:48EST on 27Oct2022 Colab Pro (The Idiot)\n",
        "#         2m42s  @18:06EST on 27Oct2022 Colab Pro (The Idiot)\n",
        "#         2m37s  @23:20EST on 27Oct2022 Colab Pro (The Idiot)\n",
        "\n",
        "# Call external get_sentimentr::get_sentimentr_values with Python loop over all novels\n",
        "\n",
        "line_ls = sentiment_df['line'].to_list()\n",
        "\n",
        "# Convert Python List of Strings to a R vector of characters\n",
        "# https://rpy2.github.io/doc/v3.0.x/html/generated_rst/pandas.html\n",
        "sentence_v = robjects.StrVector(line_ls)\n",
        "sentiment_df_r = get_sentimentr_function_r(sentence_v)\n",
        "\n",
        "# Convert rpy2.robjects.vectors.DataFrame to pandas.core.frame.DataFrame\n",
        "# https://stackoverflow.com/questions/20630121/pandas-how-to-convert-r-dataframe-back-to-pandas \n",
        "print(f'type(sentiment_df_r): {type(sentiment_df_r)}')\n",
        "temp_df = pd.DataFrame.from_dict({ key : np.asarray(sentiment_df_r.rx2(key)) for key in sentiment_df_r.names })\n",
        "print(f'type(temp_df): {type(temp_df)}')\n",
        "\n",
        "# Create new SentimentR DataFrame to save results\n",
        "# sentimentr_df = sentiment_df[['line_no', 'line']].copy(deep=True)\n",
        "sentimentr_df = pd.DataFrame()\n",
        "\n",
        "# This works for Novels New Corpus Texts\n",
        "sentimentr_df['sentimentr_jockersrinker'] = temp_df['sentimentr_jockersrinker']\n",
        "sentimentr_df['sentimentr_jockers'] = temp_df['sentimentr_jockers']\n",
        "sentimentr_df['sentimentr_huliu'] = temp_df['sentimentr_huliu']\n",
        "sentimentr_df['sentimentr_nrc'] = temp_df['sentimentr_nrc']\n",
        "sentimentr_df['sentimentr_senticnet'] = temp_df['sentimentr_senticnet']\n",
        "sentimentr_df['sentimentr_sentiword'] = temp_df['sentimentr_sentiword']\n",
        "sentimentr_df['sentimentr_loughran_mcdonald'] = temp_df['sentimentr_loughran_mcdonald']\n",
        "sentimentr_df['sentimentr_socal_google'] = temp_df['sentimentr_socal_google'] \n",
        "\n",
        "sentimentr_df.head()"
      ],
      "metadata": {
        "id": "Rz1qp9R8Ygp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentimentr_df.columns.to_list()"
      ],
      "metadata": {
        "id": "t-uFe2VyndRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "win_per = 0.1\n",
        "win_size = int(win_per * sentimentr_df.shape[0])\n",
        "\n",
        "sentimentr_model_ls = [\n",
        "    'sentimentr_jockersrinker',\n",
        "    'sentimentr_jockers',\n",
        "    'sentimentr_huliu',\n",
        "    'sentimentr_nrc',\n",
        "    'sentimentr_senticnet',\n",
        "    'sentimentr_sentiword',\n",
        "    'sentimentr_loughran_mcdonald',\n",
        "    'sentimentr_socal_google']\n",
        "\n",
        "_ = sentimentr_df[sentimentr_model_ls].rolling(win_size, center=True).mean().plot(figsize=(12,6), grid=True)"
      ],
      "metadata": {
        "id": "kcVwkXIJa7BC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save SyuzhetR Models' Sentiment Time Series\n",
        "\n",
        "save_df2csv_and_download(sentimentr_df, '_sentimentr.csv', nodate=True)"
      ],
      "metadata": {
        "id": "JwklgP5ba7BD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Option (2): Statistical ML\n",
        "\n",
        "* https://towardsdatascience.com/building-a-sentiment-classifier-using-scikit-learn-54c8e7c5d2f0"
      ],
      "metadata": {
        "id": "5lnCsFNUQZuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Naive Bayes\n",
        "\n",
        "# https://www.datacamp.com/tutorial/simplifying-sentiment-analysis-python"
      ],
      "metadata": {
        "id": "MWQCuXN1RrF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: SVM\n",
        "\n",
        "# https://www.kaggle.com/code/bansodesandeep/sentiment-analysis-support-vector-machine"
      ],
      "metadata": {
        "id": "ycOzMmcGWFwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Option (3): Connectionist: Transformers\n",
        "\n",
        "**WARNING:** This takes a LONG TIME to run to completion (~45mins).\n",
        "\n",
        "Accelerate Large Models:\n",
        "\n",
        "* https://ponder.io/faster-hugging-face-with-modin/ ***\n",
        "\n",
        "* https://huggingface.co/blog/accelerate-large-models\n",
        "\n",
        "* (Moden) https://github.com/modin-project/modin\n",
        "* (Moden+HF) https://github.com/ponder-org/ponder-blog/blob/main/Modin%20%2B%20Hugging%20Face%20Tutorial.ipynb \n",
        "\n",
        "* https://heartbeat.comet.ml/optimizing-a-huggingface-transformer-model-for-toxic-speech-detection-6d59e66f615a"
      ],
      "metadata": {
        "id": "r3YQL4FKQdBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers"
      ],
      "metadata": {
        "id": "a8yQt0LzpJTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "L5Uj6TzEFFoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelWithLMHead  # T5Base 50k\n",
        "from transformers import AutoModelForSequenceClassification, Trainer\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoModelWithLMHead\n",
        "\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "import sentencepiece"
      ],
      "metadata": {
        "id": "HLbC-Gx9pEcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create class for data preparation\n",
        "\n",
        "class SimpleDataset:\n",
        "    def __init__(self, tokenized_texts):\n",
        "        self.tokenized_texts = tokenized_texts\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.tokenized_texts[\"input_ids\"])\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return {k: v[idx] for k, v in self.tokenized_texts.items()}"
      ],
      "metadata": {
        "id": "JizomLuFsOZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HF: RoBERTa Lg 15 Datas\n",
        "\n",
        "siebert/sentiment-roberta-large-english\n",
        "\n",
        "* https://colab.research.google.com/github/chrsiebert/sentiment-roberta-large-english/blob/main/sentiment_roberta_prediction_example.ipynb"
      ],
      "metadata": {
        "id": "SVFTFSU9pVPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer and model, create trainer\n",
        "\n",
        "model_name = \"siebert/sentiment-roberta-large-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "trainer = Trainer(model=model)"
      ],
      "metadata": {
        "id": "GIQKzZKVr-aq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create list of texts (can be imported from .csv, .xls etc.)\n",
        "\n",
        "# Test\n",
        "line_ls = ['I like that','That is annoying','This is great!','Wouldn´t recommend it.']\n",
        "\n",
        "# Novel Lines\n",
        "line_ls = sentiment_df['line'].to_list()"
      ],
      "metadata": {
        "id": "C-L7j_8xsrpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize texts and create prediction data set\n",
        "tokenized_texts = tokenizer(line_ls,truncation=True,padding=True)\n",
        "pred_dataset = SimpleDataset(tokenized_texts)"
      ],
      "metadata": {
        "id": "0jtKBTZesz-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5mjnob3sMCl"
      },
      "source": [
        "%%time\n",
        "\n",
        "# NOTE: 4m00s 23:57EST on 27Oct2022 Colab Pro (The Idiot)\n",
        "#       4m18s 02:27EST on 27Oct2022 Colab Pro (The Idiot)\n",
        "\n",
        "# Run predictions\n",
        "predictions = trainer.predict(pred_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(predictions)"
      ],
      "metadata": {
        "id": "daSsWIMGSRhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3O53RHCsVd7"
      },
      "source": [
        "# Transform predictions to labels\n",
        "sentiment_ls = predictions.predictions.argmax(-1)\n",
        "labels_ls = pd.Series(sentiment_ls).map(model.config.id2label)\n",
        "scores_ls = (np.exp(predictions[0])/np.exp(predictions[0]).sum(-1,keepdims=True)).max(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "line_no_ls = list(range(len(preds)))"
      ],
      "metadata": {
        "id": "ULY4BunEQp7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhIONI7ett0q"
      },
      "source": [
        "# Create DataFrame with texts, predictions, labels, and scores\n",
        "roberta15lg_df = pd.DataFrame(list(zip(line_no_ls, line_ls,sentiment_ls,labels_ls,scores_ls)), columns=['line_no','line','roberta15lg','label','score'])\n",
        "roberta15lg_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "roberta15lg_df['label'].unique()"
      ],
      "metadata": {
        "id": "3JCLRT4dwidn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "win_per = 0.1\n",
        "win_size = int(win_per * roberta15lg_df.shape[0])\n",
        "\n",
        "_ = roberta15lg_df['sentiment'].rolling(win_size, center=True).mean().plot(grid=True)"
      ],
      "metadata": {
        "id": "MuG9zeXqwW1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Model Sentiment Time Series\n",
        "\n",
        "save_df2csv_and_download(roberta15lg_df, '_roberta15lg.csv', nodate=True)"
      ],
      "metadata": {
        "id": "Hudb1WfewW1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HF: Default DistilBERT\n",
        "\n",
        "distilbert-base-uncased-finetuned-sst-2-english\n",
        "\n",
        "* https://huggingface.co/docs/transformers/task_summary"
      ],
      "metadata": {
        "id": "QrsQK7ezzDfU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer and model, create trainer\n",
        "\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "trainer = Trainer(model=model)"
      ],
      "metadata": {
        "id": "XMOGq-X9WIbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create list of texts (can be imported from .csv, .xls etc.)\n",
        "\n",
        "# Test\n",
        "line_ls = ['I like that','That is annoying','This is great!','Wouldn´t recommend it.']\n",
        "\n",
        "# Novel Lines\n",
        "line_ls = sentiment_df['line'].to_list()"
      ],
      "metadata": {
        "id": "nFbXm_H7WbYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize texts and create prediction data set\n",
        "tokenized_texts = tokenizer(line_ls,truncation=True,padding=True)\n",
        "pred_dataset = SimpleDataset(tokenized_texts)"
      ],
      "metadata": {
        "id": "wgIseJpWWbYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylqEUWutWbYq"
      },
      "source": [
        "%%time\n",
        "\n",
        "# NOTE: 0m40s 02:49EST on 28Oct2022 Colab Pro (The Idiot)\n",
        "\n",
        "\n",
        "# Run predictions\n",
        "predictions = trainer.predict(pred_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(predictions)"
      ],
      "metadata": {
        "id": "MyAE2QZVWbYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4vzPeFoWbYq"
      },
      "source": [
        "# Transform predictions to labels\n",
        "sentiment_ls = predictions.predictions.argmax(-1)\n",
        "labels_ls = pd.Series(sentiment_ls).map(model.config.id2label)\n",
        "scores_ls = (np.exp(predictions[0])/np.exp(predictions[0]).sum(-1,keepdims=True)).max(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "line_no_ls = list(range(len(sentiment_ls)))"
      ],
      "metadata": {
        "id": "KKdY2KTVWbYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1W968Sz8WbYr"
      },
      "source": [
        "# Create DataFrame with texts, predictions, labels, and scores\n",
        "distilbert_df = pd.DataFrame(list(zip(line_no_ls, line_ls,sentiment_ls,labels_ls,scores_ls)), columns=['line_no','line','distilbert','label','score'])\n",
        "distilbert_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distilbert_df['label'].unique()"
      ],
      "metadata": {
        "id": "hMLpdjoAWbYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "win_per = 0.1\n",
        "win_size = int(win_per * distilbert_df.shape[0])\n",
        "\n",
        "_ = distilbert_df['sentiment'].rolling(win_size, center=True).mean().plot(grid=True)"
      ],
      "metadata": {
        "id": "6k09nS-BWbYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Model Sentiment Time Series\n",
        "\n",
        "save_df2csv_and_download(roberta15lg_df, '_distilbert.csv', nodate=True)"
      ],
      "metadata": {
        "id": "lUoHS481WbYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete to end"
      ],
      "metadata": {
        "id": "rAKAOg3cWIVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NLWXhlLfWISE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ygSpkRM-WIPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = pipeline(\"sentiment-analysis\")"
      ],
      "metadata": {
        "id": "WJ6daK2yzDR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test\n",
        "\n",
        "result = classifier(\"I hate you\")[0]\n",
        "print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")\n",
        "\n",
        "result = classifier(\"I love you\")[0]\n",
        "print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
      ],
      "metadata": {
        "id": "GzVhBi5ZzDO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# NOTE: 48m20s 00:25EST on 28Oct2022 Colab Pro (The Idiot)\n",
        "\n",
        "line_ls = sentiment_df['line'].to_list()\n",
        "\n",
        "distilbert_tup_ls = [(classifier(x)[0]['label'], round(classifier(x)[0]['score'],4)) for x in tq.tqdm(line_ls)]"
      ],
      "metadata": {
        "id": "15XFFGCtzDL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(distilbert_tup_ls)\n",
        "print('\\n')\n",
        "print(distilbert_tup_ls)\n",
        "print('\\n\\n')\n",
        "type(distilbert_tup_ls[0])\n",
        "print('\\n')\n",
        "print(distilbert_tup_ls[0])\n",
        "print('\\n')"
      ],
      "metadata": {
        "id": "937X8yqr7kgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_ls, prob_ls = list(zip(*distilbert_tup_ls))\n",
        "\n",
        "pred_ls = ['' ]\n",
        "\n",
        "\n",
        "print(f'label_ls: {sentiment_ls[:5]}')\n",
        "print(f'prob_ls: {prob_ls[:5]}')"
      ],
      "metadata": {
        "id": "_eJQXUjS7kcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert NEGATIVE/POSITIVE into 0/1 int values\n",
        "\n",
        "pred_ls = [1 if x=='POSITIVE' else 0 for x in label_ls]\n",
        "print(f'pred_ls: {pred_ls[:5]}')"
      ],
      "metadata": {
        "id": "KwQ8nn68BjPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform predictions to labels\n",
        "\"\"\"\n",
        "preds = predictions.predictions.argmax(-1)\n",
        "labels = pd.Series(preds).map(model.config.id2label)\n",
        "scores = (np.exp(predictions[0])/np.exp(predictions[0]).sum(-1,keepdims=True)).max(1)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "qj1Bh1I318ip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QIZq2ZW0EZk"
      },
      "source": [
        "# Create DataFrame with texts, predictions, labels, and scores\n",
        "distilbert_df = pd.DataFrame(list(zip(line_ls,pred_ls,label_ls,prob_ls)), columns=['line','distilbert','label','prob'])\n",
        "distilbert_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distilbert_df['label'].unique()"
      ],
      "metadata": {
        "id": "sk5e5A_iClYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "win_per = 0.1\n",
        "win_size = int(win_per * distilbert_df.shape[0])\n",
        "\n",
        "_ = distilbert_df['distilbert'].rolling(win_size, center=True).mean().plot(grid=True)"
      ],
      "metadata": {
        "id": "yh-tFZNVClYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Model Sentiment Time Series\n",
        "\n",
        "save_df2csv_and_download(distilbert_df, '_distilbert.csv')"
      ],
      "metadata": {
        "id": "Sz5_jzgYClYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HF: MultiBERT NLPTown\n",
        "\n",
        "nlptown/bert-base-multilingual-uncased-sentiment\n",
        "\n",
        "* https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment?text=I+like+you.+I+love+you"
      ],
      "metadata": {
        "id": "nMk2CQeViO40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer and model, create trainer\n",
        "\n",
        "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "trainer = Trainer(model=model)"
      ],
      "metadata": {
        "id": "6bPn93D-9un1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create list of texts (can be imported from .csv, .xls etc.)\n",
        "\n",
        "# Test\n",
        "# line_ls = ['I like that','That is annoying','This is great!','Wouldn´t recommend it.']\n",
        "\n",
        "# Novel Lines\n",
        "line_ls = sentiment_df['line'].to_list()"
      ],
      "metadata": {
        "id": "-EMWbKA3-CM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize texts and create prediction data set\n",
        "tokenized_texts = tokenizer(line_ls,truncation=True,padding=True)\n",
        "pred_dataset = SimpleDataset(tokenized_texts)"
      ],
      "metadata": {
        "id": "cDJG5iEu-CM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bE63EELQ-CM_"
      },
      "source": [
        "%%time\n",
        "\n",
        "# NOTE:  4m00s 23:57EST on 27Oct2022 Colab Pro (The Idiot)\n",
        "#        1m28s 01:24EST on 28Oct2022 Colab Pro (The Idiot)\n",
        "#        1m27s 02:42EST on 28Oct2022 Colab Pro (The Idiot)\n",
        "\n",
        "# Run predictions\n",
        "predictions = trainer.predict(pred_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ft9zucAJ-CM_"
      },
      "source": [
        "# Transform predictions to labels\n",
        "sentiment_ls = predictions.predictions.argmax(-1)\n",
        "labels_ls = pd.Series(sentiment_ls).map(model.config.id2label)\n",
        "scores_ls = (np.exp(predictions[0])/np.exp(predictions[0]).sum(-1,keepdims=True)).max(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "line_no_ls = list(range(len(sentiment_ls)))"
      ],
      "metadata": {
        "id": "JdA3_IeGUPff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHKwQi2k-CM_"
      },
      "source": [
        "# Create DataFrame with texts, predictions, labels, and scores\n",
        "nlptown_df = pd.DataFrame(list(zip(line_no_ls,line_ls,sentiment_ls,labels_ls,scores_ls)), columns=['line_no','line','nlptown','label','score'])\n",
        "nlptown_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlptown_df.shape"
      ],
      "metadata": {
        "id": "Xzy-u9cJUnfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlptown_df['label'].unique()"
      ],
      "metadata": {
        "id": "jBvG4K7i-CNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "win_per = 0.1\n",
        "win_size = int(win_per * nlptown_df.shape[0])\n",
        "\n",
        "_ = nlptown_df['sentiment'].rolling(win_size, center=True).mean().plot(grid=True)"
      ],
      "metadata": {
        "id": "4pxZrVAY-CNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Model Sentiment Time Series\n",
        "\n",
        "save_df2csv_and_download(nlptown_df, '_nlptown.csv', nodate=True)"
      ],
      "metadata": {
        "id": "Pffzn0hK-CNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aCdAxQXlVXb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RFEiZ1IJVXYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X_WDo8tE-XVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db_tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "# classifier = pipeline(\"sentiment-analysis\", model=\"j-hartmann/emotion-english-distilrobertabase\",max_length=512,truncation=True, tokenizer= db_tokenizer)\n",
        "classifier = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\",max_length=512,truncation=True, tokenizer= db_tokenizer)\n",
        "\n",
        "def sentiment_classifier(text):\n",
        "    classifier_results = classifier(text)[0]\n",
        "    return classifier_results['label'],classifier_results['score']"
      ],
      "metadata": {
        "id": "JkrecBMNhqqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_lines_ls)"
      ],
      "metadata": {
        "id": "9W2DvWLNMb8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5YUnUCKdM-3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stars_ls, star_prob_ls = zip(*[sentiment_classifier(x) for x in test_lines_ls])\n",
        "# stars_ls = [x[1] for x in sentiment_ls]\n",
        "stars_ls\n",
        "print(type(stars_ls))\n",
        "star_prob_ls"
      ],
      "metadata": {
        "id": "MR9KuPmji45e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_df.head()\n",
        "sentiment_df.shape"
      ],
      "metadata": {
        "id": "vD3Wf0nQkHQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "line_ls = sentiment_df['line'].to_list()\n",
        "line_ls[:5]"
      ],
      "metadata": {
        "id": "B8tAwWiukdSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%time\n",
        "\n",
        "# NOTE: 42m 9:15EST, Tues, 27 Oct 2022\n",
        "\n",
        "stars_ls, star_prob_ls = zip(*[sentiment_classifier(x) for x in line_ls])\n",
        "# stars_ls = [x[1] for x in sentiment_ls]\n",
        "stars_ls\n",
        "print(type(stars_ls))\n",
        "star_prob_ls"
      ],
      "metadata": {
        "id": "XyARd_VUkHNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_df.head()"
      ],
      "metadata": {
        "id": "FV49WL03v_Ln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "line_no_ls = list(range(len(stars_ls)))\n",
        "print(f'Length(stars_ls): {len(line_no_ls)}')"
      ],
      "metadata": {
        "id": "9cxCZptgwPXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Raw to file\n",
        "\n",
        "temp_df = pd.DataFrame({'line_no':line_no_ls, 'stars':stars_ls, 'prob':star_prob_ls})\n",
        "temp_df.head()"
      ],
      "metadata": {
        "id": "xGSN8Ljjv4gi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_df.head()"
      ],
      "metadata": {
        "id": "9i12CgCFyjQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge 'line' on 'line_no'\n",
        "\n",
        "temp_df = temp_df.merge(sentiment_df[['line_no','line']],how='left', on='line_no', suffixes=('','_y'))\n",
        "temp_df.head()"
      ],
      "metadata": {
        "id": "H7WGunpywm04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Move last column to the first\n",
        "df = pd.DataFrame(technologies)\n",
        "temp_cols=df.columns.tolist()\n",
        "new_cols=temp_cols[-1:] + temp_cols[:-1]\n",
        "df=df[new_cols]\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "wbtG3AGR1eV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reorder Columns\n",
        "\n",
        "cols_order_ls = ['line_no','line','stars','prob']\n",
        "temp_df = temp_df.reindex(columns=cols_order_ls) \n",
        "temp_df.head()"
      ],
      "metadata": {
        "id": "A9y4r6u32I-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure no Empty values\n",
        "\n",
        "temp_df['stars'].isna().sum()"
      ],
      "metadata": {
        "id": "5dHsUNh90tEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check all possible variations\n",
        "\n",
        "temp_df['stars'].value_counts()"
      ],
      "metadata": {
        "id": "APMsodURwmsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(star_prob_ls, bins=100)\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "X-0xneQWvGJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(stars_ls, bins=100)\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "oKflDh3DvZBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Star Ratings in Order\n",
        "\n",
        "star_types_ls = list(temp_df['stars'].unique())\n",
        "star_types_ls.sort(reverse=False)\n",
        "print(star_types_ls)\n",
        "\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "\n",
        "ax = temp_df.stars.value_counts().loc[star_types_ls].plot.bar()\n",
        "ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n"
      ],
      "metadata": {
        "id": "OtO5izJ53hwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fivestar2float(star_str, star_prob):\n",
        "  '''\n",
        "  INPUT: a '{n} star(s)' string rating WITH assoc probability/confidence\n",
        "  OUTPUT: a float value {n}\n",
        "\n",
        "  TODO: Weight probability into conversion\n",
        "  '''\n",
        "\n",
        "  star_fl = float(star_str.split()[0])\n",
        "\n",
        "  # TODO: Adjust based upon probability/confidence\n",
        "\n",
        "  return star_fl\n",
        "\n",
        "# Test\n",
        "\n",
        "test_str = '5 stars'\n",
        "fivestar2float(test_str, 0.3)"
      ],
      "metadata": {
        "id": "ldy8VGoNzlfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert NLPTown star ratings into Floating point\n",
        "\n",
        "temp_df['nlptown'] = temp_df['stars'].apply(lambda x: float(x.split()[0]))\n",
        "temp_df.head()"
      ],
      "metadata": {
        "id": "xcl0TzP3zQWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save to DataFrame and download copy\n",
        "\n",
        "save_df2csv_download(temp_df, '_bert-nlptown.txt')"
      ],
      "metadata": {
        "id": "xLMHucwFkHHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test\n",
        "\n",
        "data = [\"I love you\", \"I hate you\"]\n",
        "sentiment_test_ls = sentiment_pipeline(data)\n",
        "\n",
        "print(sentiment_test_ls)"
      ],
      "metadata": {
        "id": "9Wedzg-AfsCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Edge Cases\n",
        "\n",
        "edge_sentence_str = \"I'm not sure if I hate you, but I certainly don't care for your attitude young man!\"\n",
        "\n",
        "sentiment_score = sentiment_pipeline(edge_sentence_str)\n",
        "\n",
        "print(sentiment_score)"
      ],
      "metadata": {
        "id": "q0eO6yTkdLs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentiment_score[0]['label'])\n",
        "print(sentiment_score[0]['score'])"
      ],
      "metadata": {
        "id": "wBUd1FF2emFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_df.head()"
      ],
      "metadata": {
        "id": "JR2TJpyNclzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_sample_df = sentiment_df[sentiment_df['line_no'] < 20].copy()\n",
        "sentiment_sample_df.head()"
      ],
      "metadata": {
        "id": "LjYrRkRgcsTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_line_no = 19\n",
        "sentiment_sample_df.iloc[sample_line_no]"
      ],
      "metadata": {
        "id": "BmiCdy5qdzga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Line: {sentiment_sample_df.iloc[sample_line_no][\"line\"]}\\n\\n')\n",
        "\n",
        "print(sentiment_pipeline(sentiment_sample_df.iloc[sample_line_no]['line']))"
      ],
      "metadata": {
        "id": "FAVHMmk0duCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_pipeline(sentiment_sample_df.iloc[19]['line'])[0].values()"
      ],
      "metadata": {
        "id": "OZVSQNdDfFdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp_df = pd.DataFrame()\n",
        "temp_df['sentiment'], temp_df['score'] = zip(*sentiment_sample_df.apply(lambda x: sentiment_pipeline(x['line'])[0].values(), axis=1))\n",
        "temp_df.head()"
      ],
      "metadata": {
        "id": "JMv1y8WhQGGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_sample_df['sentiment'],sentiment_sample_df['score'] = zip(*sentiment_sample_df.apply(lambda x: sentiment_pipeline(x['line'])[0].values(),axis=1))\n",
        "sentiment_sample_df.head()"
      ],
      "metadata": {
        "id": "Zsk_RTB5QBPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# NOTE: 30m30s @ 16:33EST on Weds 20221026 w/GPU \n",
        "\n",
        "# CAUTION: Make sure you have selected a GPU runtime type, this will take awhile\n",
        "\n",
        "sentiment_ls = []\n",
        "\n",
        "sent_ct = len(novel_clean_ls)\n",
        "\n",
        "# for i, asentence in enumerate(novel_clean_ls):\n",
        "for asentence in tqdm(novel_clean_ls):\n",
        "  asentiment = sentiment_pipeline(asentence)\n",
        "  alabel = asentiment[0]['label']\n",
        "  if alabel == 'NEGATIVE':\n",
        "    score_sign_fl = -1.0\n",
        "  else:\n",
        "    score_sign_fl = 1.0\n",
        "\n",
        "  ascore_fl = score_sign_fl * float(asentiment[0]['score'])\n",
        "\n",
        "  # print(f'{i}/{sent_ct}')\n",
        "\n",
        "  # print(f'{i}/{sent_ct} asentiment: {asentiment[0]}')\n",
        "  # print(f'     label: {alabel}')\n",
        "  # print(f'     score: {ascore_fl}')\n",
        "\n",
        "  sentiment_ls.append(ascore_fl)"
      ],
      "metadata": {
        "id": "DRxzwRz8fWNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save to file and download copy\n",
        "\n",
        "novel_sentiments_filename = novel_name_str.split('.')[0] + '_sentiments.csv'\n",
        "\n",
        "sentiment_df['hf'] = pd.Series(sentiment_ls)\n",
        "sentiment_df.to_csv(novel_sentiments_filename)\n",
        "\n",
        "files.download(novel_sentiments_filename)"
      ],
      "metadata": {
        "id": "GUrY73VY_Ret"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HF T5 IMDB\n",
        "\n",
        "mrm8488/t5-base-finetuned-imdb-sentiment\n",
        "\n",
        "* https://huggingface.co/mrm8488/t5-base-finetuned-imdb-sentiment"
      ],
      "metadata": {
        "id": "xo_dcZlv-3pl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import AutoTokenizer, AutoModelWithLMHead\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-imdb-sentiment\")\n",
        "\n",
        "model = AutoModelWithLMHead.from_pretrained(\"mrm8488/t5-base-finetuned-imdb-sentiment\")\n",
        "\n",
        "def get_sentiment(text):\n",
        "  input_ids = tokenizer.encode(text + '</s>', return_tensors='pt')\n",
        "\n",
        "  output = model.generate(input_ids=input_ids,\n",
        "               max_length=2)\n",
        "  \n",
        "  dec = [tokenizer.decode(ids) for ids in output]\n",
        "  label = dec[0]\n",
        "  return label\n",
        "  \n",
        "get_sentiment(\"I dislike a lot that film\")\n",
        "\n",
        "# Output: 'negative'\n"
      ],
      "metadata": {
        "id": "nj6bSJb9I0cz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qe0BevtEI0aA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5Model\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "model = T5Model.from_pretrained('t5-small')\n",
        "\n",
        "input_ids = tokenizer.encode(\"Hello, my dog is cute\", return_tensors=\"pt\")  # Batch size 1\n",
        "outputs = model(input_ids=input_ids, decoder_input_ids=input_ids)\n",
        "\n",
        "last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple"
      ],
      "metadata": {
        "id": "-K9b6u_jIt9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o2BrDDVlIt4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"mrm8488/t5-base-finetuned-imdb-sentiment\"\n",
        "tokenizer = AutoModelWithLMHead.from_pretrained(model_name)\n",
        "model = AutoModelWithLMHead.from_pretrained(model_name)\n",
        "trainer = Trainer(model=model)"
      ],
      "metadata": {
        "id": "48q7zEnLEfC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create list of texts (can be imported from .csv, .xls etc.)\n",
        "\n",
        "# Test\n",
        "pred_texts = ['I like that','That is annoying','This is great!','Wouldn´t recommend it.']\n",
        "\n",
        "# Novel Lines\n",
        "pred_texts = line_ls"
      ],
      "metadata": {
        "id": "39ysafzEH-m7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize texts and create prediction data set\n",
        "tokenized_texts = tokenizer(pred_texts) # ,truncation=True,padding=True)\n",
        "pred_dataset = SimpleDataset(tokenized_texts)"
      ],
      "metadata": {
        "id": "AMwvJb3-H-m7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkb8Xc_1H-m8"
      },
      "source": [
        "%%time\n",
        "\n",
        "# NOTE:  4m00s 23:57EST on 27Oct2022 Colab Pro (The Idiot)\n",
        "#        1m28s 01:24EST on 28Oct2022 Colab Pro (The Idiot)\n",
        "\n",
        "# Run predictions\n",
        "predictions = trainer.predict(pred_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqsiuZpfH-m8"
      },
      "source": [
        "# Transform predictions to labels\n",
        "preds = predictions.predictions.argmax(-1)\n",
        "labels = pd.Series(preds).map(model.config.id2label)\n",
        "scores = (np.exp(predictions[0])/np.exp(predictions[0]).sum(-1,keepdims=True)).max(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQ_CEh8bH-m8"
      },
      "source": [
        "# Create DataFrame with texts, predictions, labels, and scores\n",
        "nlptown_df = pd.DataFrame(list(zip(pred_texts,preds,labels,scores)), columns=['line','pred','label','score'])\n",
        "nlptown_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlptown_df['label'].unique()"
      ],
      "metadata": {
        "id": "Zuo6nfNWH-m8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "win_per = 0.1\n",
        "win_size = int(win_per * nlptown_df.shape[0])\n",
        "\n",
        "_ = nlptown_df['pred'].rolling(win_size, center=True).mean().plot(grid=True)"
      ],
      "metadata": {
        "id": "Ue_Z9nFHH-m8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Model Sentiment Time Series\n",
        "\n",
        "save_df2csv_and_download(nlptown_df, '_nlptown.csv')"
      ],
      "metadata": {
        "id": "jNlxgOMvH-m8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AGKI5yXkH4FM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rbDv3SrnH4CN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def get_sentiment(text):\n",
        "  input_ids = tokenizer.encode(text + '</s>', return_tensors='pt')\n",
        "\n",
        "  output = model.generate(input_ids=input_ids,\n",
        "               max_length=2)\n",
        "  \n",
        "  dec = [tokenizer.decode(ids) for ids in output]\n",
        "  label = dec[0]\n",
        "  return label\n",
        "  \n",
        "get_sentiment(\"I dislike a lot that film\")\n",
        "\n",
        "# Output: 'negative'\n"
      ],
      "metadata": {
        "id": "tzhW9esHEe_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q9bwlxq-Ee8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM"
      ],
      "metadata": {
        "id": "0ftGBWxSADiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-imdb-sentiment\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"mrm8488/t5-base-finetuned-imdb-sentiment\")"
      ],
      "metadata": {
        "id": "Ko-KngIB-ttq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0eu7hxZ1_r28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer and model, create trainer\n",
        "\n",
        "model_name = \"mrm8488/t5-base-finetuned-imdb-sentiment\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "trainer = Trainer(model=model)"
      ],
      "metadata": {
        "id": "LLyOhslk_x9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create list of texts (can be imported from .csv, .xls etc.)\n",
        "\n",
        "# Test\n",
        "pred_texts = ['I like that','That is annoying','This is great!','Wouldn´t recommend it.']\n",
        "\n",
        "# Novel Lines\n",
        "pred_texts = line_ls"
      ],
      "metadata": {
        "id": "9zUajbLp_x9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize texts and create prediction data set\n",
        "tokenized_texts = tokenizer(pred_texts,truncation=True,padding=True)\n",
        "pred_dataset = SimpleDataset(tokenized_texts)"
      ],
      "metadata": {
        "id": "DjcV-AVG_x9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tcrs1X5_x9S"
      },
      "source": [
        "%%time\n",
        "\n",
        "# NOTE: 4m00s 23:57EST on 27Oct2022 Colab Pro (The Idiot)\n",
        "\n",
        "# Run predictions\n",
        "predictions = trainer.predict(pred_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7utEKNU_x9S"
      },
      "source": [
        "# Transform predictions to labels\n",
        "preds = predictions.predictions.argmax(-1)\n",
        "labels = pd.Series(preds).map(model.config.id2label)\n",
        "scores = (np.exp(predictions[0])/np.exp(predictions[0]).sum(-1,keepdims=True)).max(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MN9ZPcvo_x9T"
      },
      "source": [
        "# Create DataFrame with texts, predictions, labels, and scores\n",
        "nlptown_df = pd.DataFrame(list(zip(pred_texts,preds,labels,scores)), columns=['text','pred','label','score'])\n",
        "nlptown_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlptown_df['label'].unique()"
      ],
      "metadata": {
        "id": "gfP2L121_x9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "win_per = 0.1\n",
        "win_size = int(win_per * nlptown_df.shape[0])\n",
        "\n",
        "_ = nlptown_df['pred'].rolling(win_size, center=True).mean().plot(grid=True)"
      ],
      "metadata": {
        "id": "i4ZyrqIj_x9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save VADER Model Sentiment Time Series\n",
        "\n",
        "save_df2csv_and_download(nlptown_df, '_nlptown.csv')"
      ],
      "metadata": {
        "id": "tdmZhF4o_x9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "waNaiO6W_rzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4n3rYRAZ_rYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### T5\n",
        "\n",
        "mrm8488/t5-base-finetuned-span-sentiment-extraction\"\n",
        "\n",
        "* https://huggingface.co/mrm8488/t5-base-finetuned-imdb-sentiment"
      ],
      "metadata": {
        "id": "0rj20oAxbm-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[sentencepiece]"
      ],
      "metadata": {
        "id": "mznHqiVscIdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
        "\n",
        "model = \"mrm8488/t5-base-finetuned-span-sentiment-extraction\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "model = AutoModelWithLMHead.from_pretrained(model)"
      ],
      "metadata": {
        "id": "qPlsgf5wb4Cf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def get_sentiment_span(text):\n",
        "  input_ids = tokenizer.encode(text, return_tensors=\"pt\", add_special_tokens=True)  # Batch size 1\n",
        "  \n",
        "  generated_ids = model.generate(input_ids=input_ids, num_beams=1, max_length=80).squeeze()\n",
        "  \n",
        "  predicted_span = tokenizer.decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "    \n",
        "  return predicted_span\n",
        "  \n",
        "get_sentiment_span(\"question: negative context: My bike was put on hold...should have known that.... argh total bummer\")\n",
        "\n",
        "# output: 'argh total bummer'\n",
        "\n",
        "get_sentiment_span(\"question: positive context: On the monday, so i wont be able to be with you! i love you\")\n",
        "\n",
        "# output: 'i love you'\n"
      ],
      "metadata": {
        "id": "pfmKwDNjbm0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kmAJaZgVbmxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HF DistilBERT\n",
        "\n",
        "distilbert-base-uncased-finetuned-sst-2-english\n",
        "\n",
        "* https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english?text=I+like+you.+I+love+you"
      ],
      "metadata": {
        "id": "3GaOLQ2ZxZt_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
        "with torch.no_grad():\n",
        "    logits = model(**inputs).logits\n",
        "\n",
        "predicted_class_id = logits.argmax().item()\n",
        "model.config.id2label[predicted_class_id]"
      ],
      "metadata": {
        "id": "ilG4c2CTwRhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create list of texts (can be imported from .csv, .xls etc.)\n",
        "\n",
        "# Test\n",
        "pred_texts = ['I like that','That is annoying','This is great!','Wouldn´t recommend it.']\n",
        "\n",
        "# Novel Lines\n",
        "pred_texts = line_ls"
      ],
      "metadata": {
        "id": "rRqCn_FMxyYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize texts and create prediction data set\n",
        "tokenized_texts = tokenizer(pred_texts,truncation=True,padding=True)\n",
        "pred_dataset = SimpleDataset(tokenized_texts)"
      ],
      "metadata": {
        "id": "uiPP3d2UxyYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(\"Good lovely wonderfully cute\", return_tensors=\"pt\")\n",
        "# inputs = tokenizer(\"Damn bad, evil and ugly\", return_tensors=\"pt\")\n",
        "with torch.no_grad():\n",
        "    logits = model(**inputs).logits\n",
        "\n",
        "predicted_class_id = logits.argmax().item()\n",
        "model.config.id2label[predicted_class_id]"
      ],
      "metadata": {
        "id": "JSIi5SgRwRes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits"
      ],
      "metadata": {
        "id": "7pg8krgtwRby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RoBERTa Large (trained on 15 datasets)\n",
        "\n",
        "hf_model = 'siebert/sentiment-roberta-large-english'\n",
        "\n",
        "classifier_sentiment = pipeline(\"sentiment-analysis\",model=hf_model,max_length=512,truncation=True, tokenizer= db_tokenizer)\n",
        "# Test\n",
        "print(classifier_sentiment(\"I love this!\"))"
      ],
      "metadata": {
        "id": "D-hHTiT2pLDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new roberta15lg5cat  DataFrame to save results\n",
        "\n",
        "roberta15lg5cat_df = sentiment_df[['line_no', 'line']].copy(deep=True)\n",
        "roberta15lg5cat_df.head()"
      ],
      "metadata": {
        "id": "7Uxqaampq7By"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new roberta15lg5cat  DataFrame to save results\n",
        "\n",
        "roberta15lg5cat_df = sentiment_df[['line_no', 'line']].copy(deep=True)\n",
        "\n",
        "\n",
        "progress_apply(lambda x: labelscore2fl(sa_model(x), sa_model=model_name))\n",
        "   \n",
        "roberta15lg5cat_df['roberta_15lg5cat'] = pd.Series(sentiment_vader_ls) \n",
        "roberta15lg5cat_df.head()"
      ],
      "metadata": {
        "id": "mOZmZfjwpK_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qD0OKU2jpK89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3o9YzdeOpK59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HF Default: DistilBERT"
      ],
      "metadata": {
        "id": "Dw0HhcrKLwrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Dataset: List of TestSentiment Strings\n",
        "\n",
        "test_lines_ls = [\n",
        "    \"I love you.\",\n",
        "    \"You hate me.\",\n",
        "    \"I'm not sure if I hate you, but I certainly don't care for your attitude young man!\"\n",
        "]"
      ],
      "metadata": {
        "id": "hHOruWKHidv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db_tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "max_length=512,truncation=True, tokenizer= db_tokenizer\n",
        "\n",
        "# Uses default DistilBERT: distilbert-base-uncased-finetuned-sst-2-english (as of 26 Oct 2022))\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\")  "
      ],
      "metadata": {
        "id": "U7ZDIWz0R1Fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_6tVwGILiOnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HF Distil RoBERTa Base"
      ],
      "metadata": {
        "id": "tcv90GOmbyNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "db_tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\", model=\"j-hartmann/emotion-english-distilrobertabase\",max_length=512,truncation=True, tokenizer= db_tokenizer)\n",
        "\n",
        "def sentiment_classifier(text):\n",
        "\n",
        "    classifier_results = classifier(text)[0]\n",
        "    return classifier_results['label'],classifier_results['score']"
      ],
      "metadata": {
        "id": "ITNP7wA6b32u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HF RoBERTa Large"
      ],
      "metadata": {
        "id": "raoZ1R5WL1TH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# NOTE: 00m11s @12:39 on 20220301 Colab Pro \n",
        "\n",
        "sa_model = pipeline(\"sentiment-analysis\",model=\"siebert/sentiment-roberta-large-english\")\n",
        "\n",
        "print(sa_model(\"I love this!\"))"
      ],
      "metadata": {
        "id": "tKE03pCaL4G9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentiment_classifier(text):\n",
        "    classifier_results = sa_model(text)[0]\n",
        "    return classifier_results['label'],classifier_results['score']"
      ],
      "metadata": {
        "id": "8Nuhp--7NDmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "polarity_ls, star_prob_ls = zip(*[sentiment_classifier(x) for x in test_lines_ls])\n",
        "# stars_ls = [x[1] for x in sentiment_ls]\n",
        "stars_ls\n",
        "print(type(stars_ls))\n",
        "star_prob_ls"
      ],
      "metadata": {
        "id": "DDHXzisQMUyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stars_ls, star_prob_ls = zip(*[sentiment_classifier(x) for x in test_lines_ls])\n",
        "# stars_ls = [x[1] for x in sentiment_ls]\n",
        "stars_ls\n",
        "print(type(stars_ls))\n",
        "star_prob_ls"
      ],
      "metadata": {
        "id": "RKBpCbmUL4Dd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%whos"
      ],
      "metadata": {
        "id": "5v48JwJyt5ji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# NOTE: 30m30s @ 16:33EST on Weds 20221026 w/GPU \n",
        "\n",
        "# CAUTION: Make sure you have selected a GPU runtime type, this will take awhile\n",
        "\n",
        "sentiment_ls = []\n",
        "\n",
        "sent_ct = len(novel_clean_ls)\n",
        "\n",
        "# for i, asentence in enumerate(novel_clean_ls):\n",
        "for asentence in tqdm(novel_clean_ls):\n",
        "  asentiment = sentiment_classifier(asentence)\n",
        "  alabel = asentiment[0]['label']\n",
        "  if alabel == 'NEGATIVE':\n",
        "    score_sign_fl = -1.0\n",
        "  else:\n",
        "    score_sign_fl = 1.0\n",
        "\n",
        "  ascore_fl = score_sign_fl * float(asentiment[0]['score'])\n",
        "\n",
        "  # print(f'{i}/{sent_ct}')\n",
        "\n",
        "  # print(f'{i}/{sent_ct} asentiment: {asentiment[0]}')\n",
        "  # print(f'     label: {alabel}')\n",
        "  # print(f'     score: {ascore_fl}')\n",
        "\n",
        "  sentiment_ls.append(ascore_fl)"
      ],
      "metadata": {
        "id": "BepuvYt1L4AQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lw07vPyTL39p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot"
      ],
      "metadata": {
        "id": "ZLMS4igchaNH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dilate SentimentR Time Series"
      ],
      "metadata": {
        "id": "jOSzjorPiHOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all files with only one model sentiment time series\n",
        "\n",
        "novel_root_str = novel_name_str.split('.')[0]\n",
        "\n",
        "sentimentr_filename_csv = f'{novel_root_str}_sentimentr.csv'\n",
        "sentimentr_all_df = pd.read_csv(sentimentr_filename_csv, index_col=[0])\n",
        "sentimentr_model_ls = list(set(sentimentr_all_df.columns.to_list()) - set(['line_no','line']))\n",
        "\n",
        "sentimentr_all_df.head()\n",
        "sentimentr_all_df.info()\n",
        "sentimentr_model_ls"
      ],
      "metadata": {
        "id": "6I9Go1LliHEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "win_per = 0.1\n",
        "win_size = int(win_per * sentimentr_all_df.shape[0])\n",
        "\n",
        "_ = sentimentr_all_df[sentimentr_model_ls].rolling(win_size, center=True).mean().plot(figsize=(12,8), grid=True)"
      ],
      "metadata": {
        "id": "4Bk-QDkXjs1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentimentr_all_df.shape"
      ],
      "metadata": {
        "id": "z8l81Dz3lhLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'SentimentR rows: {sentimentr_all_df.shape[0]}')\n",
        "print(f'    Others rows: {vader_file_df.shape[0]}\\n')\n",
        "\n",
        "print(f'     Difference: {sentimentr_all_df.shape[0] - vader_file_df.shape[0]}')\n"
      ],
      "metadata": {
        "id": "Aw9IlNa_iHAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# define standard scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# retrieve just the numeric input values\n",
        "data = sentimentr_all_df.values[:,:]\n",
        "\n",
        "# perform a robust scaler transform of the dataset\n",
        "data = scaler.fit_transform(data)\n",
        "\n",
        "# convert the array back to a dataframe\n",
        "sentimentr_all_norm_df = pd.DataFrame(data)\n",
        "sentimentr_all_norm_df.columns = sentimentr_model_ls\n",
        "\n",
        "# summarize\n",
        "print(sentimentr_all_norm_df.describe())\n",
        "\n",
        "# histograms of the variables\n",
        "sentimentr_all_norm_df.hist()\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "NbT9mltJjEDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SentimentR 8 Model Plot"
      ],
      "metadata": {
        "id": "KwNi6y0Jl9DQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "win_per = 0.1\n",
        "win_size = int(win_per * sentimentr_all_norm_df.shape[0])\n",
        "\n",
        "model_cols = [\n",
        "    'sentimentr_nrc',\n",
        "    'sentimentr_jockers',\n",
        "    'sentimentr_socal_google',\n",
        "    'sentimentr_huliu',\n",
        "    'sentimentr_senticnet',\n",
        "    'sentimentr_sentiword',\n",
        "    'sentimentr_loughran_mcdonald',\n",
        "    'sentimentr_jockersrinker']\n",
        "\n",
        "_ = sentimentr_all_norm_df[model_cols].rolling(win_size, center=True).mean().plot(figsize=(12,8), grid=True)"
      ],
      "metadata": {
        "id": "z6__3m9WkbX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Model Sentiment Time Series\n",
        "\n",
        "save_df2csv_and_download(sentimentr_all_norm_df, '_sentimentr8norm.csv', nodate=True)"
      ],
      "metadata": {
        "id": "S_zFM8u2mXUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merge Model Data"
      ],
      "metadata": {
        "id": "UDsfc0AIJhy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name_ls = ['vader',\n",
        "                 'syuzhetr',\n",
        "                 'sentimentr',\n",
        "                 'roberta15lg',\n",
        "                 'distilbert',\n",
        "                 'nlptown']"
      ],
      "metadata": {
        "id": "23O6NGq4Jrzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "novel_root_str = novel_name_str.split('.')[0]\n",
        "\n",
        "# Get all files with only one model sentiment time series\n",
        "vader_filename_csv = f'{novel_root_str}_vader.csv'\n",
        "vader_file_df = pd.read_csv(vader_filename_csv)\n",
        "vader_file_ls = vader_file_df['vader'].to_list()\n",
        "\n",
        "distilbert_filename_csv = f'{novel_root_str}_distilbert.csv'\n",
        "distilbert_file_df = pd.read_csv(distilbert_filename_csv)\n",
        "# distilbert_file_ls = distilbert_file_df['distilbert'].to_list()\n",
        "distilbert_file_ls = distilbert_file_df['sentiment'].to_list()\n",
        "\n",
        "nlptown_filename_csv = f'{novel_root_str}_distilbert.csv'\n",
        "nlptown_file_df = pd.read_csv(nlptown_filename_csv)\n",
        "# distilbert_file_ls = distilbert_filnlptown_file_dfe_df['distilbert'].to_list()\n",
        "nlptown_file_ls = nlptown_file_df['sentiment'].to_list()\n",
        "\n",
        "roberta15lg_filename_csv = f'{novel_root_str}_distilbert.csv'\n",
        "roberta15lg_file_df = pd.read_csv(roberta15lg_filename_csv)\n",
        "# distilbert_file_ls = roberta15lg_file_df['distilbert'].to_list()\n",
        "roberta15lg_file_ls = roberta15lg_file_df['sentiment'].to_list()\n",
        "\n",
        "# Append to Syuzhet with 4 models sentiment time series\n",
        "syuzhetr_filename_csv = f'{novel_root_str}_syuzhetr.csv'\n",
        "syuzhetr_file_df = pd.read_csv(syuzhetr_filename_csv, index_col=[0])\n",
        "\n",
        "sentiment_all_df = syuzhetr_file_df.copy(deep=True)\n",
        "sentiment_all_df['vader'] = vader_file_ls\n",
        "sentiment_all_df['distilbert'] = distilbert_file_ls\n",
        "sentiment_all_df['nlptown'] = nlptown_file_ls\n",
        "sentiment_all_df['roberta15lg'] = roberta15lg_file_ls\n",
        "\n",
        "sentiment_all_df.head()"
      ],
      "metadata": {
        "id": "xfLFrkYtci7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_all_df.columns.to_list()"
      ],
      "metadata": {
        "id": "j9VElCrvfjh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "win_per = 0.1\n",
        "win_size = int(win_per * sentiment_all_df.shape[0])\n",
        "\n",
        "model_cols = [\n",
        "    'syuzhetr_syuzhet',\n",
        "    'syuzhetr_bing',\n",
        "    'syuzhetr_afinn',\n",
        "    'syuzhetr_nrc',\n",
        "    'vader',\n",
        "    'distilbert',\n",
        "    'nlptown',\n",
        "    'roberta15lg']\n",
        "\n",
        "_ = sentiment_all_df[model_cols].rolling(win_size, center=True).mean().plot(figsize=(12,8), grid=True)"
      ],
      "metadata": {
        "id": "sMW0mv51cioF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get list matching model *.csv files\n",
        "\n",
        "file_model_ls = []\n",
        "\n",
        "rootdir = \".\"\n",
        "file_root = novel_name_str.split('.')[0]\n",
        "print(file_root)\n",
        "regex = re.compile(\"{}_[^-]*\\.csv$\".format(file_root))\n",
        "# regex = re.compile('.*_.*\\.csv$')\n",
        "\n",
        "for root, dirs, files in os.walk(rootdir):\n",
        "  for file in files:\n",
        "    if regex.match(file):\n",
        "      for amodel in model_name_ls:\n",
        "        if amodel in file:\n",
        "           file_model_ls.append(file)\n",
        "\n",
        "print('Matching files:\\n')\n",
        "[x for x in file_model_ls]"
      ],
      "metadata": {
        "id": "BajNw2UOKyry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_nosentimentr_ls = []\n",
        "\n",
        "sentimentr_fl = False\n",
        "\n",
        "for afile in file_model_ls:\n",
        "  if '_sentimentr.csv' in afile:\n",
        "    print('found sentimentr')\n",
        "    sentimentr_fl= True\n",
        "  else:\n",
        "    file_nosentimentr_ls.append(afile)\n",
        "\n",
        "file_nosentimentr_ls"
      ],
      "metadata": {
        "id": "r7vF_WwMPFvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_first = file_nosentimentr_ls[0]\n",
        "print(f'file_first: {file_first}')\n",
        "merged_df = pd.read_csv(file_first, index_col=None)\n",
        "\n",
        "for file_next in file_nosentimentr_ls[1:]:\n",
        "  print(f'file_next: {file_next}')\n",
        "  new_df = pd.read_csv(file_next, index_col=None)\n",
        "  merged_df = pd.merge(merged_df, new_df, on='line_no', how='left')"
      ],
      "metadata": {
        "id": "jYipowqkOvzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.info()"
      ],
      "metadata": {
        "id": "9t5W6_XPZXPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.head()"
      ],
      "metadata": {
        "id": "q3DePd85OuqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "file_nosentimentr_ls = []\n",
        "\n",
        "sentimentr_fl = False\n",
        "\n",
        "for afile in file_model_ls:\n",
        "  if '_sentimentr.csv' in afile:\n",
        "    print('found sentimentr')\n",
        "    sentimentr_fl= True\n",
        "  else:\n",
        "    file_nosentimentr_ls.append(afile)\n",
        "\n",
        "file_nosentimentr_ls\n",
        "\n",
        "\n",
        "# merging two csv files\n",
        "sentiment_all_df = pd.concat(map(pd.read_csv, file_nosentimentr_ls), ignore_index=True, axis=1)\n",
        "sentiment_all_df.head()"
      ],
      "metadata": {
        "id": "ModCcwSPM-qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing pandas\n",
        "import pandas as pd\n",
        "\n",
        "# merging two csv files\n",
        "df = pd.concat(map(pd.read_csv, ['mydata.csv', 'mydata1.csv']), ignore_index=True)"
      ],
      "metadata": {
        "id": "Q65qYH0JM-iQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VDMbaMI3M_lN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#combine all files in the list\n",
        "combined_csv = pd.concat([pd.read_csv(fname) for fname in file_model_ls])\n",
        "#export to csv\n",
        "combined_csv.to_csv( \"combined_csv.csv\", index=False, encoding='utf-8-sig')"
      ],
      "metadata": {
        "id": "lmR6-3pUKRU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "smuguYTgJgtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Review\n",
        "\n",
        "sentiment_df.head()"
      ],
      "metadata": {
        "id": "enCv2gFpf2jR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_cols_ls = list(sentiment_df.columns)\n",
        "sentiment_cols_ls.remove('sentence_no')\n",
        "sentiment_cols_ls.remove('sentence_str')\n",
        "sentiment_cols_ls"
      ],
      "metadata": {
        "id": "nyBoB2lkHVCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalize Data\n",
        "\n",
        "* https://stackoverflow.com/questions/64882432/sklearn-preprocessing-standardscaler-valueerror-expected-2d-array-got-1d-array"
      ],
      "metadata": {
        "id": "i4VxEj0oCrum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_all_df.head()\n",
        "sentiment_all_df.info()"
      ],
      "metadata": {
        "id": "jDxE-EgUgxCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# define standard scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# retrieve just the numeric input values\n",
        "data = sentiment_all_df.values[:, 2:]\n",
        "\n",
        "# perform a robust scaler transform of the dataset\n",
        "data = scaler.fit_transform(data)\n",
        "\n",
        "# convert the array back to a dataframe\n",
        "sentiment_all_norm_df = pd.DataFrame(data)\n",
        "\n",
        "# summarize\n",
        "print(sentiment_all_norm_df.describe())\n",
        "\n",
        "# histograms of the variables\n",
        "sentiment_all_norm_df.hist()\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "E3ZqFc2Ogaka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_all_cols_ls = sentiment_all_df.columns.to_list()\n",
        "sentiment_all_norm_df.columns = sentiment_all_cols_ls[2:]\n",
        "sentiment_all_norm_df.head()"
      ],
      "metadata": {
        "id": "S1BA8OAuhjmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Non-SentimentR 8 Model Plot"
      ],
      "metadata": {
        "id": "4ZM_mT4mmDJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "win_per = 0.1\n",
        "win_size = int(win_per * sentiment_all_norm_df.shape[0])\n",
        "\n",
        "model_cols = [\n",
        "    'syuzhetr_syuzhet',\n",
        "    'syuzhetr_bing',\n",
        "    'syuzhetr_afinn',\n",
        "    'syuzhetr_nrc',\n",
        "    'vader',\n",
        "    'distilbert',\n",
        "    'nlptown',\n",
        "    'roberta15lg']\n",
        "\n",
        "_ = sentiment_all_norm_df[model_cols].rolling(win_size, center=True).mean().plot(figsize=(12,8), grid=True)"
      ],
      "metadata": {
        "id": "jsgOwkc3hbJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Model Sentiment Time Series\n",
        "\n",
        "save_df2csv_and_download(sentiment_all_norm_df, '_nonsentimentr8norm.csv', nodate=True)"
      ],
      "metadata": {
        "id": "EYVXEi1lm2DB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# End"
      ],
      "metadata": {
        "id": "xt7IRpA0hbFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BRL3ct3YhXW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize Sentiment Time Series across different models\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "sentiment_norm_df = pd.DataFrame()\n",
        "\n",
        "scaler = StandardScaler() \n",
        "for acol in sentiment_cols_ls:\n",
        "  t = t.reshape(-1,1)\n",
        "  sentiment_norm_df[acol] = scaler.fit_transform(sentiment_df[acol].to_numpy()) \n",
        "# sentiment_norm_df = pd.DataFrame({sentiment_cols_ls[0]: scaled_values[:, 0], sentiment_cols_ls[1]: scaled_values[:, 1]})\n",
        "sentiment_norm_df.insert(0, 'sentence_no', sentiment_df['sentence_no'])\n",
        "sentiment_norm_df.insert(1, 'sentence_str', sentiment_df['sentence_str'])\n",
        "sentiment_norm_df.head()"
      ],
      "metadata": {
        "id": "ogQBd_4aGx4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save to file and download copy\n",
        "\n",
        "novel_sentiments_norm_filename = novel_name_str.split('.')[0] + '_sentiments_norm.csv'\n",
        "\n",
        "sentiment_norm_df['hf'] = pd.Series(sentiment_ls)\n",
        "sentiment_norm_df.to_csv(novel_sentiments_norm_filename)\n",
        "\n",
        "files.download(novel_sentiments_norm_filename)"
      ],
      "metadata": {
        "id": "iuW8xyOhJeyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Static Plots"
      ],
      "metadata": {
        "id": "jtHqqHksCwEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "win_per = .10\n",
        "\n",
        "win_size = int(sentiment_df.shape[0] * win_per)\n",
        "\n",
        "sentiment_norm_df[['vader','hf']].rolling(window=win_size, center=True).mean().plot(figsize=(20,10), grid=True)\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "ogP1P3P2IKWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interactive Plots\n",
        "\n",
        "* https://plotly.com/python/time-series/"
      ],
      "metadata": {
        "id": "0anJtGHfCz6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using graph_objects\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px"
      ],
      "metadata": {
        "id": "8BE79lihJlKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Interactive Plotly Time Series Charts\n",
        "\n",
        "import pandas as pd\n",
        "# df = pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/finance-charts-apple.csv')\n",
        "\n",
        "# fig = go.Figure([go.Scatter(y=sentiment_norm_df['vader'].rolling(window=win_size, center=True).mean())])\n",
        "# fig.show()\n",
        "\n",
        "fig = go.Figure(fig.add_traces(\n",
        "                 data=px.line(sentiment_norm_df, x='sentence_no', y='vader', hover_name=\"sentence_str\")._data))\n",
        "fig = go.Figure(fig.add_traces(\n",
        "                 data=px.line(sentiment_norm_df, x='sentence_no', y='hf', hover_name=\"sentence_str\")._data))\n",
        "fig.update_layout(title='Diachronic Sentiment Analysis', showlegend=False)\n",
        "fig.show();"
      ],
      "metadata": {
        "id": "I7pmxTSGJCHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TlPi2mK1M9O_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}